{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJtd7Ddds11R"
   },
   "source": [
    "# Instruction\n",
    "1. Check if you have the following folders:\n",
    "   \n",
    "\n",
    "*   data/models/cnn\n",
    "*   data/augment\n",
    "\n",
    "\n",
    "   \n",
    "2. Check if you have the following files \n",
    "   \n",
    "*   data/training.csv\n",
    "*   data/test.csv\n",
    "*   data/IdLookupTable.csv\n",
    "\n",
    "3. Make sure you **do not have** the following files\n",
    "\n",
    "*   data/augment/augdat_*.p\n",
    "*   data/models/cnn/*.p \n",
    "*   data/models/cnn/*.h5 \n",
    "*   data/submission_data.p\n",
    "*   data/transformed_data.p\n",
    "*   data/processed_data.p\n",
    "\n",
    "4. In the model description section(few cells below) choose your option\n",
    "\n",
    "5. On Edit -> Notebook settings -> choose Hardware accelerator 'GPU' and Runtime-shape \"High RAM\"\n",
    "\n",
    "6. Runtime -> Run all (follow instructions to authenticate)\n",
    "\n",
    "If the Colab stops, simply restart the code by \"Run all\", it will start where it left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yNKAQaAfeFK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd drive/MyDrive/Colab\\ Notebooks/cct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwTCDOQMu7Ic"
   },
   "outputs": [],
   "source": [
    "base = Path('drive/MyDrive/Colab\\ Notebooks/cct/')\n",
    "sys.path.insert(0, str(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWKbyNVmCmbP"
   },
   "outputs": [],
   "source": [
    "!apt-get install build-essential cmake\n",
    "!apt-get install libopenblas-dev liblapack-dev \n",
    "!pip3 install face_recognition\n",
    "!pip3 install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jskfIE8nlwq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modules.models import CNN\n",
    "from modules.models import Means\n",
    "from modules.MultiModel import MultiModel\n",
    "from modules.source_data import SourceData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfiIKpp7bWM9"
   },
   "source": [
    "# Model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqjkWXpVuQP_"
   },
   "outputs": [],
   "source": [
    "# Is it debug mode?\n",
    "# If running debug remmeber to remove all created files afterwards\n",
    "debug = False  # Setting it to True will set off a quick run\n",
    "\n",
    "# Response names for all the parameters\n",
    "response_names = ['left_eye_center', 'right_eye_center', 'left_eye_inner_corner', \n",
    "                  'left_eye_outer_corner', 'right_eye_inner_corner',  \n",
    "                  'right_eye_outer_corner', 'left_eyebrow_inner_end', \n",
    "                  'left_eyebrow_outer_end', 'right_eyebrow_inner_end', \n",
    "                  'right_eyebrow_outer_end', 'nose_tip', 'mouth_left_corner', \n",
    "                  'mouth_right_corner', 'mouth_center_top_lip', \n",
    "                  'mouth_center_bottom_lip']\n",
    "\n",
    "if not debug:\n",
    "  # Which model are we going to use for this\n",
    "  my_model = CNN\n",
    "else:\n",
    "  # Run the means\n",
    "  my_model = Means\n",
    "\n",
    "# Augment the data or not\n",
    "do_augment = True\n",
    "\n",
    "# Prefix to give all saved files \n",
    "prefix = 'center_compare'\n",
    "\n",
    "option = 1 \n",
    "if option == 1:\n",
    "  # option 1: Do not do any centering just run on raw data\n",
    "  centering_params = {'skip_center': True}\n",
    "elif option == 2:\n",
    "  # option 2: Center image but do not scale and when there are multiple faces, pick the one in the middle\n",
    "  centering_params = {'skip_center': False, 'do_scale': False, \n",
    "                      'pick_center_image': True}\n",
    "elif option == 3:\n",
    "  # option 3: Center image, scale and when there are multiple faces, pick the one in the middle\n",
    "  centering_params = {'skip_center': False, 'do_scale': True, \n",
    "                      'pick_center_image': True}\n",
    "elif option == 4:\n",
    "  # option 4: Center image, scale and when there are multiple faces, pick the biggest\n",
    "  centering_params = {'skip_center': False, 'do_scale': True, \n",
    "                      'pick_center_image': False}\n",
    "else:\n",
    "  raise ValueError('You must choose of these options')\n",
    "print('Centering params are %s' % str(centering_params))\n",
    "\n",
    "if not debug: \n",
    "  # Augmentation parameters for each response name\n",
    "  augmentation_params = {'left_eye_inner_corner': {'num_transforms': 12},\n",
    "                          'left_eye_outer_corner': {'num_transforms': 12},\n",
    "                          'right_eye_inner_corner': {'num_transforms': 12},\n",
    "                          'right_eye_outer_corner': {'num_transforms': 12},\n",
    "                          'left_eyebrow_inner_end': {'num_transforms': 12},\n",
    "                          'left_eyebrow_outer_end': {'num_transforms': 12},\n",
    "                          'right_eyebrow_inner_end': {'num_transforms': 12},\n",
    "                          'right_eyebrow_outer_end': {'num_transforms': 12},\n",
    "                          'mouth_left_corner': {'num_transforms': 12},\n",
    "                          'mouth_right_corner': {'num_transforms': 12},\n",
    "                          'mouth_center_top_lip': {'num_transforms': 12},\n",
    "                          'mouth_center_bottom_lip': {'num_transforms': 8},\n",
    "                          'nose_tip': {'num_transforms': 8},\n",
    "                          'left_eye_center': {'num_transforms': 8},\n",
    "                          'right_eye_center': {'num_transforms': 8}}\n",
    "else:\n",
    "  # Augmentation parameters for each response name\n",
    "  augmentation_params = {'left_eye_inner_corner': {'num_transforms': 1},\n",
    "                          'left_eye_outer_corner': {'num_transforms': 1},\n",
    "                          'right_eye_inner_corner': {'num_transforms': 1},\n",
    "                          'right_eye_outer_corner': {'num_transforms': 1},\n",
    "                          'left_eyebrow_inner_end': {'num_transforms': 1},\n",
    "                          'left_eyebrow_outer_end': {'num_transforms': 1},\n",
    "                          'right_eyebrow_inner_end': {'num_transforms': 1},\n",
    "                          'right_eyebrow_outer_end': {'num_transforms': 1},\n",
    "                          'mouth_left_corner': {'num_transforms': 1},\n",
    "                          'mouth_right_corner': {'num_transforms': 1},\n",
    "                          'mouth_center_top_lip': {'num_transforms': 1},\n",
    "                          'mouth_center_bottom_lip': {'num_transforms': 1},\n",
    "                          'nose_tip': {'num_transforms': 1},\n",
    "                          'left_eye_center': {'num_transforms': 1},\n",
    "                          'right_eye_center': {'num_transforms': 1}}\n",
    "                      \n",
    "\n",
    "# Which points datapoints to eliminate in which group\n",
    "# eliminate_params = {0: ['nose_tip', 'mouth_center_bottom_lip'], 1: []}\n",
    "eliminate_params = None\n",
    "\n",
    "# Response names to model: None signifies all\n",
    "# model_responses = ['left_eye_center', ]\n",
    "model_responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKfesxvMaIAi"
   },
   "outputs": [],
   "source": [
    "# Random seed for splitting train/cv/test data\n",
    "seed = 27652\n",
    "\n",
    "# Source the data here\n",
    "sd = SourceData(debug=debug, center_params=centering_params)\n",
    "\n",
    "# As this is the final run we will include the test data as part of the training\n",
    "data = sd.source_data(combine_train_test=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FtJKjCHktE0"
   },
   "source": [
    "Create models using the optimal parameters and save the best model. Best is the one with minimum validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfMZ2HLoj6wD"
   },
   "outputs": [],
   "source": [
    "# Create a set of models using the means model\n",
    "mmo = MultiModel(my_model, prefix=prefix, \n",
    "                 eliminate=eliminate_params, \n",
    "                 augment_params=augmentation_params,\n",
    "                 response_names=model_responses)\n",
    "\n",
    "# Fit the models\n",
    "train_data = data['train']\n",
    "cv_data = data['cv']\n",
    "mmo.fit(train_data, cv_data, fit_params_dict={'epochs': 300},\n",
    "        optimizer_params_dict={'name': 'sgd', 'learning_rate': 0.00005, 'momentum': 0.8}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebgafA904oG"
   },
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UWogkhu5tD_"
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "  # Create a set of models using ny_nodel\n",
    "  mmo = MultiModel(my_model, prefix=prefix)\n",
    "  # Load the \n",
    "  mmo.load(response_names)\n",
    "\n",
    "# Create the submission file\n",
    "submmision_data = sd.source_test_csv()\n",
    "mmo.create_submission(submission_data=submmision_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3iRYCKQc9vQ"
   },
   "source": [
    "# Test data predictions and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWpXNw03Uq4t"
   },
   "outputs": [],
   "source": [
    "  if 0:\n",
    "    # Data does not exist then pull it from file\n",
    "    try:\n",
    "      # Make predictions on test data\n",
    "      test_data = preprocessed['test_labeled']\n",
    "    except NameError:\n",
    "      # Source the data with or without augmenation(default is with)\n",
    "      data, _ = source_data()\n",
    "\n",
    "      # Transform all the images in the DataFrame\n",
    "      preprocessed = preprocess_multiple_data(\n",
    "          data_dict=data, drop_orig=True, debug=False, save_test=True)\n",
    "      \n",
    "      # Make predictions on test data\n",
    "      test_data = preprocessed['test_labeled']\n",
    "\n",
    "    # Need to compare against the original y values\n",
    "    test_data = test_data.rename(columns={'y': 'translated_y', 'orig_y': 'y'})\n",
    "\n",
    "    # If \n",
    "    try:\n",
    "      pred, metrics = mmo.predict(test_data)\n",
    "    except NameError:\n",
    "      # Create a set of models using the means model\n",
    "      mmo = MultiModel(my_model, prefix=prefix)\n",
    "      mmo.load(response_names)\n",
    "\n",
    "      pred, metrics = mmo.predict(test_data)\n",
    "\n",
    "    print(metrics)\n",
    "    print('Average error %.2f' % np.mean([x for x in metrics.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDhioM0Kd5BK"
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "  # Amongst the pair of values, which one is the worst?\n",
    "  pred['delta'] = pred[['y_invert', 'y_true']].apply(lambda x: np.max(np.abs(x[0]-x[1])), axis=1)\n",
    "\n",
    "  # Create a histogram of those values\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.hist(pred['delta'].values, bins=96)\n",
    "  plt.xlabel('Error in prediction')\n",
    "  plt.ylabel('Count of error')\n",
    "  plt.title('Histogram of prediction error')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkt9UcP1k-TM"
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "  # Create a DataFrame of values in the  tail region\n",
    "  pred = pred.sort_values(by='delta', ascending=False, axis=0)\n",
    "  tail_df = pred[:10]\n",
    "  for cnt, x in enumerate(tail_df['X'].values):\n",
    "    plt.figure()\n",
    "    plt.imshow(x)\n",
    "    if cnt > 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KvIsDkVvlWh"
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "  # Create a DataFrame of values in the  core region\n",
    "  tail_df = pred[10:50]\n",
    "  for cnt, x in enumerate(tail_df['X'].values):\n",
    "    plt.figure()\n",
    "    plt.imshow(x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1-gqasiUbOB2i-94oEkJKR7ysgmLSDTBd",
     "timestamp": 1627008681727
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
